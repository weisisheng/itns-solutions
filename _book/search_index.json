[
["index.html", "Solutions for ‘Introdcution To The New Statistics’ Preface Interactive Exercises R tutorial General Setup", " Solutions for ‘Introdcution To The New Statistics’ Peter Baumgartner 2020-03-07 Preface This interactive book is a companion book for Introduction to the New Statistics (abbreviated itns). The two types of additional material it offers are: interactive exercises with solutions, R tutorials for the end-of-chapter exercises. Interactive Exercises I have built the interactive exercises in this book with H5P.org. H5P stands for HTML5 Package, a free and open-source content collaboration framework based on JavaScript. With H5P, it is easy for everyone to display, create, share, and reuse interactive HTML5 content through a standard browser. At the moment (February 2020), there are 45 different kinds of exercises (“content types” in H5P parlance). H5P content can be injected in any platform that supports embedded content (iframes). There exist already integrations for Learning Management Systems like Canvas, Brightspace, Blackboard, Moodle and other systems that support the standard for Learning Tools Interoperability (See also: How LTI works) In addition H5P has plugins developed for WordPress, Moodle, Drupal and several other publishing systems. If you don’t want to manage your own platform as a developer of interactive exercises you can also pay $57/month using H5P.com software as a paid service (SaaS). I am using my German WordPress blog Gedankensplitter with the required free H5P-plugin to develop interactive content presented in this book. I have tested these exercises at the backend site of my WordPress installation and — if they work as intended — I have published them here into the R-publishing system bookdown via embedded code generated by the H5P WordPress plugin. As bookdown is built on top of R Markdown, it is very suitable also to include the R tutorial. Even if the technical handling process of developing interactive content with H5P is documented excellently and therefore relatively quickly done, there is another — more educational — difficulty. What kind of exercises are valuable to learn the content presented in Introduction to The New Statistics (abbreviated: itns)? Yes, there are some quizzes included in the book which one “only” has to transfer into H5P content. With the emphasis on “only,” I want to address some difficulty resulting from converting static textual exercises into dynamic computer evaluated interactions. So, for instance, it is laborious to design the right solution for open-ended answers. But besides this more technical problem, there is also the more pressing issue to create valuable educational interactions with the tools provided by H5P. Instead of using just traditional educational interactions on the web (most notably multiple-choice and fill-the-gap), I have experimented with a wide variety of different types of exercises. Furthermore, I have applied the concept of multiple representations by developing various tasks for the same content. R tutorial The second component of this companion book to itns is a demonstration of how to carry out the statistical challenges presented in the end-of-chapter exercises using the programming language R, a free, open-source software environment for statistical computing and graphics. This part of the book is a tutorial. It applies R not only for the statistical analysis and visualization but also for other necessary practical tasks like data cleaning, data transformation, and data modeling. I assume some basic knowledge with the R language as I will not explain installation procedures and basic R commands. I am using RStudio, the prevalent free integrated development environment (IDE), and occasionally I mention RStudio specifics. But these hints are rare and not essential to understand and to replicate my suggested R procedures. I will conclude the preface with a warning: I was intrigued by itns as a new generation of a statistical textbook. Criticizing the standard null hypothesis significance testing (NHST) by using the estimation approach based on confidence intervals (CIs), meta-analysis, and integrating Open Science met my thinking and preferences. But the truth is: I am neither a statistical expert nor an R wizard! My professional background is in instructional design and technology-enhanced learning (TEL). I am using the challenge to write this book as my particular vehicle to learn and practice statistical analysis and the R programming language. In this sense, I am just another learner applying the elaboration technique so splendid and convincing explained in “Make it stick: The science of successful learning” (Brown, Roediger, and McDaniel 2014). General Setup Global Options ### setting up working environment ### for details see: https://yihui.name/knitr/options/ knitr::opts_chunk$set( echo = T, message = T, error = T, warning = T, comment = &#39;##&#39;, highlight = T, prompt = T, strip.white = T, tidy = T ) Installing and loading R packages &gt; ### accompanying R package: https://github.com/gitrman/itns &gt; if (!require(&quot;itns&quot;)) { + remotes::install_github(&quot;gitrman/itns&quot;, build = TRUE, build_opts = c(&quot;--no-resave-data&quot;, &quot;--no-manual&quot;)) + library(&quot;itns&quot;) + } ## Loading required package: itns &gt; ### https://www.tidyverse.org/ &gt; if (!require(&quot;tidyverse&quot;)) { + install.packages(&quot;tidyverse&quot;, repos = &quot;http://cran.wu.ac.at/&quot;) + library(tidyverse) + } ## Loading required package: tidyverse ## ── Attaching packages ────────────────────────────────────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.0 ✓ purrr 0.3.3 ## ✓ tibble 2.1.3 ✓ dplyr 0.8.4 ## ✓ tidyr 1.0.2 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.5.0 ## ── Conflicts ───────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() &gt; ### above command installed and loaded the core tidyverse packages: ggplot2: data visualisation tibble: a modern &gt; ### take on data frames tidyr: data tidying readr: data import (csv, tsv, fwf) purrr: functional R programming &gt; ### dplyr: data (frame) manipulation stringr: string manipulation forcats: working with categorial varialbes &gt; &gt; &gt; ### to calculate mode: &gt; if (!require(&quot;modeest&quot;)) { + install.packages(&quot;modeest&quot;, repos = &quot;http://cran.wu.ac.at/&quot;) + library(modeest) + } ## Loading required package: modeest ## Registered S3 method overwritten by &#39;rmutil&#39;: ## method from ## print.response httr &gt; # I am going to use the `janitor` package for calculating table totals &gt; if (!require(&quot;janitor&quot;)) { + install.packages(&quot;janitor&quot;, repos = &quot;http://cran.wu.ac.at/&quot;) + library(janitor) + } ## Loading required package: janitor ## ## Attaching package: &#39;janitor&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## chisq.test, fisher.test &gt; ### install &#39;checkr&#39; to test code submissions in learnr (and potentially other) tutorials there is also a problem &gt; ### in line 639 in &#39;vignettes/checkr.Rmd&#39; I have changed version hoping that 0.6 is unsensible to the &#39;older&#39; CRAN &gt; ### version (0.4) and does not update &gt; if (!require(&quot;checkr&quot;)) { + remotes::install_github(&quot;petzi53/checkr2&quot;, build = TRUE, build_opts = c(&quot;--no-resave-data&quot;, &quot;--no-manual&quot;)) + library(checkr) + } ## Loading required package: checkr &gt; if (!require(&quot;itns&quot;)) { + remotes::install_github(&quot;gitrman/itns&quot;, build = TRUE, build_opts = c(&quot;--no-resave-data&quot;, &quot;--no-manual&quot;)) + library(&quot;itns&quot;) + } Theme adaption for the graphic display with ggplot2 &gt; my_theme &lt;- theme_light() + theme(plot.title = element_text(size = 10, face = &quot;bold&quot;, hjust = 0.5)) &gt; theme(plot.background = element_rect(color = NA, fill = NA)) + theme(plot.margin = margin(1, 0, 0, 0, unit = &quot;cm&quot;)) ## List of 2 ## $ plot.background:List of 5 ## ..$ fill : logi NA ## ..$ colour : logi NA ## ..$ size : NULL ## ..$ linetype : NULL ## ..$ inherit.blank: logi FALSE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_rect&quot; &quot;element&quot; ## $ plot.margin : &#39;margin&#39; num [1:4] 1cm 0cm 0cm 0cm ## ..- attr(*, &quot;valid.unit&quot;)= int 1 ## ..- attr(*, &quot;unit&quot;)= chr &quot;cm&quot; ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;theme&quot; &quot;gg&quot; ## - attr(*, &quot;complete&quot;)= logi FALSE ## - attr(*, &quot;validate&quot;)= logi TRUE References "],
["research-questions.html", "Chapter 1 Asking and Answering Research Questions 1.1 Interactive Video 1.2 Glossary 1.3 Research Process 1.4 Meta-Analysis 1.5 Reporting (Accordion) 1.6 Quizzes and Assessment", " Chapter 1 Asking and Answering Research Questions If you prefer to use for the exercises your phone or tablet, scan the associated QR code. 1.1 Interactive Video 1.1.1 Research questions 1.1.2 Meta-analysis 1.1.3 Open Science 1.2 Glossary 1.2.1 Accordion 1.2.2 Drag words 1.2.3 Memory Game 1.2.4 Fill in the Blanks 1.3 Research Process 1.3.1 Drag &amp; Drop 1.3.2 Form Wizard 1.3.3 Interactive Video see Video under heading of ‘Interactive Video’ 1.3.4 Order Phases of the Research Process (Summary) 1.3.5 Steps of the Estimation Plan (Summary) 1.4 Meta-Analysis 1.4.1 Explore Forest Plot (Image Hotspots) 1.4.2 Find Point Estimates in Forest Plot (Find Hotspots) 1.5 Reporting (Accordion) 1.6 Quizzes and Assessment 1.6.1 Quiz 1 1.6.2 Quiz 2 1.6.3 Poll Intuitions (True/False) 1.6.4 Assessment "],
["research-fundamentals.html", "Chapter 2 Research Fundamentals: Don’t Fool Yourself 2.1 Interactive Video 2.2 Glossary 2.3 Fundamental Research Concepts 2.4 Quizzes and Assessment", " Chapter 2 Research Fundamentals: Don’t Fool Yourself 2.1 Interactive Video 2.2 Glossary 2.2.1 Accordion 2.2.2 Dialog Cards 2.2.3 Drag words Part I 2.2.4 Drag words Part II 2.2.5 Flash Cards 2.3 Fundamental Research Concepts 2.3.1 Sample and Population (Find Hotspots) 2.3.2 DFY: Don’t fool yourself (Drag Words) 2.3.3 Measurement (Multiple Choice) 2.3.4 Important Concepts I (Dropdown) 2.3.5 Important Concepts II (Fill in the Blanks) 2.4 Quizzes and Assessment 2.4.1 Quiz 1 2.4.2 Quiz 2 2.4.3 Quiz 3 "],
["picturing-describing-data.html", "Chapter 3 Picturing &amp; Describing Data 3.1 End-of-Chapter Exercises 3.2 Put some indidual scores in context 3.3 Compare positive &amp; negative affect of a participant 3.4 Religious Beliefs 3.5 Working with the oiginal Excel data set", " Chapter 3 Picturing &amp; Describing Data 3.1 End-of-Chapter Exercises 3.1.1 Calculating decriptive statistics 3.1.1.1 Description The following table contains a set of ACT scores from a sample of college students. The ACT is a standardized college-readiness exam taken by many U.S. students; scores can range fro 1 to 36. Student ACT \\((X~i~ - M)\\) \\((X~i~ - M)^2\\) 1 26 2 24 3 28 4 31 5 20 6 27 7 18 8 17 9 21 10 29 11 24 Total Location: Calculate the mean (M), median, and mode for this sample. Spread: Calculate the standard deviation (s), range and interquartile range for this sample. For s, fill in the two columns on the right in the table above, then use the formula to calculate s yourself. 3.1.1.2 Solution &gt; ACT &lt;- c(26, 24, 28, 31, 20, 27, 18, 17, 21, 29, 24) With summary there is a covenient command in R to print out some of the most important descriptive statistics. &gt; summary(ACT) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 17.00 20.50 24.00 24.09 27.50 31.00 The summary command does not display mode, variance, standard deviation and IQR. With the exception of mode all these command are included in the base R installation. To calculate the mode we need modeest, a package for mode estimation. In R there are for the same task often several solution possible. I will add some of them. Parameter Command in R Value Mean M round(mean(ACT),2) 24.09 Median (V1) median(ACT) 24 Median (V2) fivenum(ACT)[3] 24 quantile (V3) quantile(ACT)[[3]] 24 Mode mfv(ACT) 24 Variance round(var(ACT),2) 21.29 s round(sd(ACT),2) 4.61 Range (V1) range(ACT) 17, 31 Range (V2) fivenum(ACT)[c(1,5)] 17, 31 IQR IQR(ACT) 7 Q1,Q3 fivenum(ACT)[c(2,4)] 20.5, 27.5 To populate the table programmatically I will convert ACT to a data frame ACT_df = as.data.frame(ACT) resp. in a tibble as tibbles has some advantages over data frames. (The tibble package is part of the tidyverse.) &gt; ### Calling `as_tibble()` on a vector is discouraged, because the behavior is likely to change in the future. I &gt; ### am using instead the `enframe` command &gt; ACT_df &lt;- enframe(ACT, name = &quot;Student&quot;, value = &quot;ACT&quot;) &gt; ## using the pipe operator (https://magrittr.tidyverse.org/) using mutate from the dplyr package which is also &gt; ## included in tidyverse &gt; ACT_df &lt;- ACT_df %&gt;% mutate(`Xi-M` = round(ACT_df$ACT - mean(ACT_df$ACT), 2), `(Xi-M)^2` = round(`Xi-M`^2, 2)) &gt; &gt; &gt; ACT_df &lt;- ACT_df %&gt;% adorn_totals(&quot;row&quot;) # janitor package to add totals &gt; ACT_df[12, 3] &lt;- &quot; &quot; # summing up Xi-M does not make sense &gt; &gt; &gt; knitr::kable(ACT_df, booktabs = TRUE, caption = &quot;ACT Scores for Exercise 1 of Chapter 3&quot;, align = &quot;rrrr&quot;) Table 3.1: ACT Scores for Exercise 1 of Chapter 3 Student ACT Xi-M (Xi-M)^2 1 26 1.91 3.65 2 24 -0.09 0.01 3 28 3.91 15.29 4 31 6.91 47.75 5 20 -4.09 16.73 6 27 2.91 8.47 7 18 -6.09 37.09 8 17 -7.09 50.27 9 21 -3.09 9.55 10 29 4.91 24.11 11 24 -0.09 0.01 Total 265 212.93 This table has not the same information as in the book p.505. At first I tried to build it similar but this complicates everything as it is not suitable for using R. 3.1.2 Loading our first data set 3.1.2.1 Inspecting the itns-package In the task description of the itns-book you will find the following passage (p.68): From the book website, load the College_Survey_1 data set. This is a large data set containing data from a wide-ranging survey of college students. You can read the code book for the data file to get a better sense of the different variables measured. But you do not need to load the file into R as there are all these data sets already included in the itns package, which we already have loaded into R. In this package the data sets are already cleaned and made compatibel with the R language. This is a great advantage and helps us to concentrate just on the exercises. As in real life you will not have your data already cleaned, I will in the last section of this chapter explain some of the necessary procedures you whould normally apply to every new data set you want to work with. In the documentation the name of the College_Survey_1 data set in the itns pacakge is college_survey1. When you have already the package attached you can find this out with the command ls(\"package:itns\") our better in RStudio with the console command help(package = itns). (Our you can also go to the “Packages” tab and search for “itns”) The former displays the objects of the package in your RMarkdown file, the latter opens up the documentation page in the RStudio help tab. &gt; ls(&quot;package:itns&quot;) # list all objects of the itns package ## [1] &quot;altruism_happiness&quot; &quot;anchor_estimate&quot; &quot;anchor_estimate_ma&quot; &quot;body_well&quot; ## [5] &quot;campus_involvement&quot; &quot;clean_moral_johnson&quot; &quot;clean_moral_schall&quot; &quot;cohensd_rm&quot; ## [9] &quot;college_survey1&quot; &quot;college_survey2&quot; &quot;dana&quot; &quot;emotion_heartrate&quot; ## [13] &quot;exam_scores&quot; &quot;flag_priming_ma&quot; &quot;home_prices&quot; &quot;home_prices_holdout&quot; ## [17] &quot;labels_flavor&quot; &quot;math_gender_iat&quot; &quot;math_gender_iat_ma&quot; &quot;natsal&quot; ## [21] &quot;organic_moral&quot; &quot;pen_laptop1&quot; &quot;pen_laptop2&quot; &quot;power_performance_ma&quot; ## [25] &quot;rattan&quot; &quot;religion_sharing&quot; &quot;religious_belief&quot; &quot;self_explain_time&quot; ## [29] &quot;sleep_beauty&quot; &quot;stickgold&quot; &quot;study_strategies&quot; &quot;thomason1&quot; ## [33] &quot;thomason2&quot; &quot;thomason3&quot; &quot;videogame_aggression&quot; Further on I will only describe the RStudio version. RStudio is the most productive R IDE (Integrated Development Environment) I know about. It is free and works with many platforms. You can download the lastest version at https://www.rstudio.com/products/rstudio/download/#download To detect the name of the variables (= columns) and the data structure we use the RStudio View command: &gt; # remove first &#39;#&#39; from next line to get code working View(college_survey1) # opens the data set in the RStudio &gt; # data browser 3.1.2.2 Reading the codebook for college-survey1 Even the data set is already included in the itns package, you still will need to go to the companion website and download the data set as you will need the code book to understand what all the variables and codes mean. To save you time I have it included here. But keep in mind that the variable names in the college-survey1 are slighty different than in College_Survey_1. The main difference is that the R variables are not capitalized. As R is case sensitive, we need always to look up the correct names for the R variables (columns). Survey of College Attitudes This is data from survey of college students. The survey was put together as a classroom project, and measures a number of different constructs. Note that not all participants elected to answer all items–blanks represent scales that were not completed by that participant. ID - Gender : Self-reported gender, Male or Female Gender_Code: 1 for Female, 2 for Male Age: Self-reported age in years School_Year - Self reported, First-year, sophomore, junior, senior, or post-bac School_Year_Code: 1 = first-year, 2 = sophomore,3 = junior, 4 = senior, 5 = post-bac Transfer: No if student did not transfer to current school, Yes if student transferred to current school Transfer_Code: 0 = not a transfer student, 1 = transfer student Student_Athlete: self-reported, non-athlete, or athlete in or out of season Student_Athlete_Code: 0 = not a student athlete, 1 = student athlete Wealth_SR: Single item: “Considering all the others students you’ve met here at SCHOOL NAME how would you rank yourself in terms of wealth?” rated on scale from 1 (well below average) to 5 (well above average) GPA: Self-reported GPA, 0-4point scale ACT: Self-reported ACT score Subjective_Well_Being: Average of 5-item satisfaction with life scale by Deiner, rated on scale from 1 (strongly disagree) to 7 (strongly agree). Sample item: “I am satisfied with my life”. See http://internal.psychology.illinois.edu/~ediener/SWLS.html Positive_Affect: Average of 10-item scale of frequency of experiencing positive emotions over the past week, rated on a scale from 1 (verly slightly to not at all) to 5 (extremely). Sample items: Enthusiastic, Proud, Inspired…Part of the PANAS scale. Negative_Affect : Average of 10-item scale of frequency of experiencing negative emotions over the past week. Rated on scale from 1( very slightly to not at all) to 5 (extrmely). Sample items: Nervous, irritable, hostile, etc&gt;0. Part of the PANAS scale. See http://booksite.elsevier.com/9780123745170/Chapter%203/Chapter_3_Worksheet_3.1.pdf Relationship_Confidence- Average of a 7-item scale of confidence in having romantic relationships, rated on a scale from 1 (not at all characteristic of me) to 5 (very characteristic of me). Sample item: \"I am a good partner for a romantic relationship) Exercise - Exercise score on the Godin Leisure-Time Exercise Quetionnaire. This asks participants to rate how often during a regular week they engage in strenuous, moderate, or light exercise. A total score is calculated as 9strenuous + 5moderate + 3*light. See http://dapa-toolkit.mrc.ac.uk/documents/en/God/Godin_Leisure-Time_Exercise_Q.pdf Academic_Motivation_Intrinsic - Average score on 6-item measure of intrinsic acadmic motivation. Participants respond to the prompt “Why do you go to college” and respond to each item with a rating from 1 (does not correpond at all) to 7 (corresponds exactly). Sample item: “For the intense feelings I experience when I am communicating my own ideas to others”. See http://www.er.uqam.ca/nobel/r26710/LRCS/scales/emec_en.doc Academic_Motivation_Extrinsic - Average score on 7-item measure of extinsic acadmic motivation. Participants respond to the prompt “Why do you go to college” and respond to each item with a rating from 1 (does not correpond at all) to 7 (corresponds exactly). Sample item: “Because with only a high-school degree I would not find a high-paying job later one”. See http://www.er.uqam.ca/nobel/r26710/LRCS/scales/emec_en.doc Academic_Motivation_Amotivation - Average score on 2-item measure of extinsic acadmic motivation. Participants respond to the prompt “Why do you go to college” and respond to each item with a rating from 1 (does not correpond at all) to 7 (corresponds exactly). Sample item: “Honestly, I don’t know; I really feel that I am wasting my time in school.”. See http://www.er.uqam.ca/nobel/r26710/LRCS/scales/emec_en.doc Inteligence_Value - Measure of degree to which participant values intelligence, rated on a scale from 1 (strongly agree) to 5 (strongly agree). Sample item: “Intelligence greatly contributes to success in life.”. See Raven_Score - % correct out of 8 items on the Raven Progressive Matrix Scale, a scale of logical thinking/IQ. Adapted from https://www.raventest.net/. 3.1.3 Distribution of positive affect scores Now we can start to visualize the different distribution as described in 2a-e (pp.68f.) 3.1.3.1 Visualize the distribution Visualize the distribution of positive affect scores (Positive_Affect). Students rate how often they experience each of 10 positive emotions, using a scale from 1 (not at all) to 5 (extremely), then the score is the average of the 10 ratings. The reference in the codebook to the PANAS scale is not valid anymore. But I found an explanation and a template with the questionaire on the toolsHero website. Following the recommendation by the paper Getting Started with the New Statistics in R (p.7) I am going to use the ggplot2 package. See details to the command at the ggplot2 web page. 3.1.3.2 Histogram &gt; ggplot(college_survey1, aes(positive_affect)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 8 rows containing non-finite values (stat_bin). After executing the above command ggplot2 gives us a notification and a warning message: 3.1.3.2.1 Addressing the notification It is important to experiment with different values of the binwidth parameter to get a feeling about the effects of the visualization of the distribution. I chose a binwidth of .1, meaning that I will get 40 bins as the scale goes from 1-5. 3.1.3.2.2 Addressing the warning ggplot2 warns us that there are data missing in 8 records. Look at the RStudio data browser and scroll down to check it. You will see that at the end of the file, starting with record number 231 some data are missing (“not availabe” = NA). In our next visualization we tell R to remove these missing values silently and to use the binwidth of .1. &gt; ggplot(college_survey1, aes(positive_affect)) + geom_histogram(binwidth = 0.1, na.rm = TRUE) # always experiment with the binwidth 3.1.3.3 Dot plot For more information about this command see: https://ggplot2.tidyverse.org/reference/geom_dotplot.html &gt; ggplot(college_survey1, aes(positive_affect)) + geom_dotplot(binwidth = 0.1, na.rm = TRUE, method = &quot;histodot&quot;, + stackratio = 1.1) + coord_fixed(ratio = 2) It is a notorious problem with dot plots produced by ggplot2 that the y-axis is normalized and displays meaningless values between 0 and 1. I found recently a workaround posted at StackOverflow. &gt; # library(ggplot2) ## already loaded with tidyverse &gt; library(dplyr) ## already loaded with tidyverse &gt; library(ggExtra) &gt; &gt; yheight = 20 # taken from the histogramm &gt; # basic dotplot (binwidth = the accuracy of the data) &gt; dotchart = ggplot(college_survey1, aes(x = positive_affect), dpi = 600) &gt; dotchart = dotchart + geom_dotplot(binwidth = 0.1, method = &quot;histodot&quot;, dotsize = 1, fill = &quot;blue&quot;, na.rm = TRUE) &gt; &gt; # use coor_fixed(ratio=binwidth*dotsize*max frequency) to setup the right y axis height. &gt; dotchart = dotchart + coord_fixed(ratio = 0.1 * yheight) &gt; &gt; # add tick mark on y axis to reflect frequencies. Note yheight is max frequency. &gt; dotchart = dotchart + scale_y_continuous(limits = c(0, 1), expand = c(0, 0), breaks = seq(0, 1, 1/yheight), labels = seq(0, + yheight)) &gt; &gt; # remove x y lables and remove vertical grid lines &gt; dotchart = dotchart + labs(x = NULL, y = NULL) + theme_bw() + removeGridX() &gt; dotchart 3.1.3.4 Interpretation Describe the distribution you see, noting the location, spread, and shape, and the number of major peaks. &gt; summary(college_survey1$positive_affect) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1.200 3.000 3.400 3.433 3.900 5.000 8 &gt; s &lt;- round(sd(college_survey1$positive_affect, na.rm = TRUE), 2) &gt; s ## [1] 0.71 Positive affect is relatively normally distributed—there is a single, strong peak and the distribution is fairly symmetrical. The mean score of 3.43 is a little above the midpoint of the 1–5 scale, indicating that students on average feel moderate levels of positive affect. There is wide spread, with s = 0.71 and scores ranging across almost the full range. 3.1.4 Distribution of student age The next exercises are similar. I am going to produce just the histograms. Visualize and describe the distribution of student age. Note the skew and the high outliers. Note the mean and median and explain why they differ. Would it make sense to delete outliers? Explain. &gt; ggplot(college_survey1, aes(age)) + geom_histogram(binwidth = 1, na.rm = TRUE) # note the chosen bandwith &gt; summary(college_survey1$age) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 18.00 19.00 20.00 21.79 22.00 59.00 4 &gt; s &lt;- round(sd(college_survey1$age, na.rm = TRUE), 2) &gt; s ## [1] 5.56 There is strong positive skew, with most students aged 18–22, but a long upper tail of ages ranging up to nearly 60. The mean (21.8 years) is most affected by the outliers, and the median (20 years) is probably a more useful indicator of location. The skew causes the mean to be larger than the median. The college has mostly younger students, but also some older returning and non-traditional students. Removing outliers would misrepresent the sample, by omitting those older students. They should be removed only if there is reason to examine the subgroup of younger students, perhaps aged up to about 32. 3.1.5 Distribution of exercise scores Again the URL in the code book does not exist anymore but you can download the article dircetly from Gaston Godin’s web page. Visualize the distribution of exercise scores, which are calculated as 9×strenuous + 5×moderate + 3×light, where students report the number of times per week they engage in strenuous, moderate, and light exercise. There is an extreme outlier. What would that extreme value mean? Would it make sense to delete it? Explain. If you do delete it, what happens to the standard deviation? Test your prediction. &gt; ggplot(college_survey1, aes(exercise)) + geom_histogram(binwidth = 15, na.rm = TRUE) &gt; summary(college_survey1$exercise) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.00 2.75 22.00 54.78 61.00 1810.00 23 &gt; s &lt;- round(sd(college_survey1$exercise, na.rm = TRUE), 2) &gt; s ## [1] 135.28 There is considerable positive skew. Most students report relatively little exercise, but the right tail pulls the mean (54.7) way above the median (22). The extreme outlier of 1,810 corresponds to engaging in strenuous exercise 201 times per week, which is hardly credible. Most likely, the student didn’t understand the question, made an error, or gave a non-serious answer, so there is good reason to delete this outlier, which would decrease the SD. If it is deleted, s decreases from 135 to 64.7. As always, if you delete an outlier report how, when, and why the descision was made to remove it. We can easy find the number of the record (= row) of this outlier when we in the RStudio data browser the column exercise are sorting from high to low. &gt; outlier &lt;- college_survey1$exercise[138] &gt; college_survey1$exercise[138] &lt;- NA &gt; ggplot(college_survey1, aes(exercise)) + geom_histogram(binwidth = 10, na.rm = TRUE) # different bindwidth as before &gt; summary(college_survey1$exercise) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.00 2.50 20.00 46.77 61.00 385.00 24 &gt; s &lt;- round(sd(college_survey1$exercise, na.rm = TRUE), 2) &gt; college_survey1$exercise[138] &lt;- outlier # restore outlier to get original data frame &gt; s ## [1] 64.72 3.1.6 Distribution of Raven scores and GPA Visualize the distribution of Raven scores (Raven_Score), which are the proportion correct on a short 8-item IQ test. Next, visualize the distribution of GPA. Compare the two distributions. Why might they differ in shape? By now this should already a routine operation. &gt; ggplot(college_survey1, aes(raven_score/100)) + geom_histogram(binwidth = 0.05, na.rm = TRUE) &gt; summary(college_survey1$raven_score/100) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.0000 0.2500 0.3750 0.3771 0.5000 0.8750 26 &gt; s &lt;- round(sd(college_survey1$raven_score/100, na.rm = TRUE), 2) &gt; s ## [1] 0.2 &gt; ggplot(college_survey1, aes(gpa)) + geom_histogram(binwidth = 0.1, na.rm = TRUE) &gt; summary(college_survey1$gpa) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.800 3.000 3.400 3.343 3.800 4.000 6 &gt; s &lt;- round(sd(college_survey1$gpa, na.rm = TRUE), 2) &gt; s ## [1] 0.51 Raven IQ Scores are relatively normally distributed. The mean is .377, median is .375, and standard deviation is .199. In contrast, GPA is very strongly negatively skewed, with scores stacked up near 4.0, the top of the GPA scale. A long tail extends down to very low values of GPA. The mean is 3.34, median is 3.40, and SD is 0.51. Different variables can have different distributions, even for the same sample. However, we expect IQ and GPA to be related, so it’s perhaps surprising that their two distributions are so different. The GPA distribution, with most of the scores concentrated in the 3.5–4.0 range, suggests that the testing is not sufficiently challenging to distinguish among students in the upper parts of the distribution. You may have heard of grade inflation occurring in the last one or two decades. Search online for “college grade inflation”, or similar, and you should easily find statistics describing a dramatic increase in the proportion of A grades in recent years, and interesting discussion about likely causes and consequences. 3.1.7 Presentation of the gender distribution In this data set (of the original Excel file, pb), Gender is coded as female = 1 and male = 2. ESCI calculates that the mean is M = 1.28 for Gender. Does this make sense? How might you better summarize gender in this sample? No! It doesn’t make sense to calculate a mean for data on a nominal scale. It’s often convenient to represent nominal data with numerical codes, but it’s important to remember that these codes have no numerical meaning, and thus calculation of most descriptive statistics doesn’t make sense. Nominal data can be summarized with frequencies (175 females and 68 males) and relative frequencies (72.0% females and 28.0% males). This important difference between numbers and nominal data is in the itns data set already considered and therefore changed. We have two variables Females and Males. These so-called factor variables require a different proceeding: &gt; n &lt;- table(college_survey1$gender) &gt; res &lt;- cbind(n, round(prop.table(n) * 100, 1)) &gt; colnames(res) &lt;- c(&quot;Count&quot;, &quot;%&quot;) &gt; res ## Count % ## Female 175 72 ## Male 68 28 3.2 Put some indidual scores in context 3.2.1 Female student on the Ravens measure One female participant achieved a score of .875 on the Raven measure. What is her z score? We can calculate the z score manually with the formula (x - mean(x)) / sd(x) or use the command scale() The parenthesis in (x-mean(x)) / sd(x) is important because the precedence of the operators in x-mean(x) / sd(x) would be wrong. We also have to remove the NA values. &gt; M &lt;- mean(college_survey1$raven_score, na.rm = TRUE)/100 &gt; s &lt;- sd(na.omit(college_survey1$raven_score))/100 &gt; (0.875 - M)/s ## [1] 2.499761 With X = .875, M = .377, s = .199 we calculate z = 2.50. This student is tied for top score in the sample. All these commands are using vectors, e.g. the take all values of the column raven_score. In inspecting the data frame we notice that .875 is the first value. So we can also use the command scale(). To prevent to print out the long list of values we write scale()[1] &gt; scale(college_survey1$raven_score)[1] ## [1] 2.499761 3.2.2 The same student in GPA The same female participant has a GPA of 3.9. What is her z score for GPA? On which measure (GPA or Raven) is she more unusual? &gt; college_survey1$gpa[1] ## [1] 3.9 &gt; mean(college_survey1$gpa, na.rm = TRUE) ## [1] 3.343038 &gt; sd(na.omit(college_survey1$gpa))[1] ## [1] 0.5083248 &gt; scale(college_survey1$gpa)[1] ## [1] 1.095681 With X = 3.9, M = 3.34, s = 0.508, we calculate z = 1.10. This participant stands out more on the IQ measure (z = 2.50) than on GPA (z = 1.10). 3.3 Compare positive &amp; negative affect of a participant One participant scored a 2 for Positive Affect and a 2 for Negative Affect. Even though these raw scores are the same, they are very different within the context of the whole study. Express both as z scores and percentiles. Which score is more unusual? Sorting either the column positive_affect or negative_affect we see that the particpant of row 154 is the one who scored a 2 for Positive Affect and a 2 for Negative Affect. But we should not use the direct 3.3.0.1 Positive Affect &gt; college_survey1$positive_affect[154] ## [1] 2 &gt; mean(college_survey1$positive_affect, na.rm = TRUE) ## [1] 3.433404 &gt; sd(na.omit(college_survey1$positive_affect)) ## [1] 0.7135492 &gt; scale(college_survey1$positive_affect)[154] ## [1] -2.008837 3.3.0.2 Negative Affect &gt; college_survey1$negative_affect[154] ## [1] 2 &gt; mean(college_survey1$negative_affect, na.rm = TRUE) ## [1] 2.322043 &gt; sd(na.omit(college_survey1$negative_affect)) ## [1] 0.7530565 &gt; scale(college_survey1$negative_affect)[154] ## [1] -0.4276473 3.3.0.3 Comparison: Positive with Negative Affect For positive affect: X = 2.0, M = 3.43, s = 0.71 and we calculate z = −2.01. For negative affect: X = 2.0, M = 2.32, s = 0.75 and we calculate z = −0.43. The positive affect score is more unusual than the negative affect score, within this sample. 3.3.1 Determine outliers A z score of more than 3 or less than −3 is sometimes considered an outlier. By this standard, would the participant who is 59 qualify as an outlier for age? Would the participant who reported an exercise score of 1,810 qualify as an outlier for exercise? &gt; college_survey1$age[69] ## [1] 59 &gt; mean(college_survey1$age, na.rm = TRUE) ## [1] 21.78661 &gt; sd(na.omit(college_survey1$age)) ## [1] 5.555721 &gt; scale(college_survey1$age)[69] ## [1] 6.69821 &gt; college_survey1$exercise[138] ## [1] 1810 &gt; mean(college_survey1$exercise, na.rm = TRUE) ## [1] 54.78182 &gt; sd(na.omit(college_survey1$exercise)) ## [1] 135.2829 &gt; scale(college_survey1$exercise)[138] ## [1] 12.97443 For age, X = 59, M = 21.8, s = 5.56 and we calculate z = 6.69, which is an extreme outlier! For exercise, X = 1,810, M = 54.8, s = 135.3 and we calculate z = 12.97, which is a very extreme outlier! 3.4 Religious Beliefs Let’s look at some data about religious beliefs. The religious_belief data set on the book website has data from a large online survey in which participants were asked to report, on a scale from 0 to 100, their belief in the existence of God. 3.4.1 Estimate the distribution First, sketch the distribution you think this variable will have. Consider the relative frequencies of people you expect to strongly believe in God (high end of the scale), to strongly not believe in God (low end of the scale), or to be unsure (middle of the scale). &gt; summary(religious_belief$belief_in_gd) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.00 2.00 50.00 49.39 100.00 100.00 The lowest 25% percent do not believe in god and the highest 25% do believe completely (100%) in god. I predicted an approximately flat distribution, with similar frequencies of strong, moderate, and non-believers. 3.4.2 Graphic of the religious data Now make a picture of these data and describe it in words. &gt; ggplot(religious_belief, aes(belief_in_gd)) + geom_histogram(binwidth = 10, na.rm = TRUE) The distribution is clearly bimodal: Most respondents answered 100 (strongly believe in God) or 0 (strongly disbelieve in God), and relatively few participants gave scores between the extremes. My prediction was quite wrong. This is a good illustration of multiple peaks (two in this case) indicating distinct groups of participants. 3.4.3 Evaluate the different location parameters Does the mean do a good job representing this data picture? Does the median? The mode? Why do all these measures of location seem to fail? With such a strongly bimodal distribution, none of the three measures of location does a great job of representing the distribution. Both the mean (48.2) [49.39] and median (50) represent an intermediate level of belief that is actually rare; reporting only one mode would be misleading. The best non-graphical strategy would be to describe the distribution in words and report the location of each mode: one at 0 level of belief, the other at 100. 3.5 Working with the oiginal Excel data set 3.5.1 Load data set from Excel file In RStudio there are several options to import data files via the menu available. I generally prefer .csv files using the option From Text (readr). readr is a package included in tidyverse. In the import data window you can preview the first 50 data. The R program code is displayed in the bottom right section of the window, so that you can automate this step. This is not only time saving but also necessary for better reproducibility. As we already have loaded readr and do not want to display the dataset in RStudio automatically we only need the second line of the program code. &gt; college1 &lt;- read_csv(&quot;_Material/AllEOC_Exercises/College_Survey_1/College_Survey_1.csv&quot;) ## Parsed with column specification: ## cols( ## .default = col_double(), ## Gender = col_character(), ## Shool_Year = col_character(), ## Transfer = col_character(), ## Student_Athlete = col_character(), ## Exercise = col_character(), ## Raven_Score = col_character() ## ) ## See spec(...) for full column specifications. After loading the dataset you will see in the console window the parameters readr has used to parse the column specification. This all is done normally correctly, automatically, and hassle-free. But it is import to check these parsing decisions made by readr. Ignore the strange [31m and [39m strings in the printed output. These are the internal characters for changing text to red color. This is the way readr warns you that there are character columns in the table. You can convert them to factor variable but this is with extra columns for Gender (Gender_Code), Schoolyear (Schoolyear_Code), and Student_Athlete (Student_Athlete_Code) already done. Remains Exercise and Rave_Score. Both columns contain numbers and should display like the default mode in the format col_double() which is the R code for double precision floating point numbers. We have to inspect these two columns more in detail: Use the command View(college1) from the console or display the data set by choosing it form the tab Environment on the right upper window. The reason for the wrong format is that the variable Exercise contains ‘-’ and Raven_Score has ‘%’-signs added after the numbers. I will deal with this problem later, when we are going to use one of this variables. During the inspection of the data set I noticed two other minor problems: The last few rows — from row 244 – 247 are emtpy, respectively filled witn NAs (“not available”). We may delete these rows. There is a typo in the column name Shool_Year which should be School_Year. &gt; college1 &lt;- college1[1:243, ] # delete empty rows &gt; colnames(college1)[5] &lt;- &quot;School_Year&quot; 3.5.2 Problems with the Exercise variable With the Exercise variable we cannot use the above ggplot2 commands to display the histogram. Try it out and you will get the following error message: Error: StatBin requires a continuous x variable: the x variable is discrete. Perhaps you want stat=“count”? The reason is a wrong format of the Exercise variable. Instead of numbers there are some dashes in some of the records. But the dash ‘-’ is a character, so the whole column is convert to a character variable. The meaning of the dash sign is that there was no exercise reported. To distinguish it from NA (= not available) which means the student did not answer the question, no exercise was coded unfortunately with a ‘-’ instead of a 0. For questions of reproducibility it is always a good idea not to change the original data set. Therefore I will create a new variable Exercise_Code, convert all dashes to 0 and format the column as.double(). &gt; college1$Exercise_Code &lt;- college1$Exercise # copy column &gt; college1 &lt;- college1[, c(1:18, 24, 19:23)] # reorder Exercise_Code after Exercise &gt; college1$Exercise_Code[college1$Exercise_Code == &quot;-&quot;] &lt;- 0 # set all &#39;-&#39; to 0 &gt; college1$Exercise_Code &lt;- as.double(college1$Exercise_Code) # convert column from character to double ## Warning: NAs introduced by coercion The last line spits out the message “NAs introduced by coercion”. This is a kind of error message saying that some number conversion did not succeed. Instead converting number strings to numbers NAs were created. To find this error was a little bit tricky but finally it turned out that the extreme outlier in row 138 with 1,810.00 was the problem: The comma in this big figure is interpreted as a character. We could change this unique value but I will provide a general solution taken from StackOverflow. We have to delete the “destroyed” column “Exercise_Code” and start again with the conversion process. In the real praxis we would change simply the code junk above and run the whole program again. &gt; college1 &lt;- college1[, c(1:18, 20:24)] # delete column `Exercise_Code` &gt; college1$Exercise_Code &lt;- college1$Exercise # create it again &gt; college1 &lt;- college1[, c(1:18, 24, 19:23)] # reorder Exercise_Code after Exercise &gt; college1$Exercise_Code[college1$Exercise_Code == &quot;-&quot;] &lt;- 0 # set all &#39;-&#39; to 0 &gt; &gt; ## and now the new line: deleting commas and converting to double precision floating point numbers &gt; college1$Exercise_Code &lt;- as.double(gsub(&quot;,&quot;, &quot;&quot;, college1$Exercise_Code)) 3.5.3 Problems with the Ravens_Score Here we are confronted with a similar problem. The “%” sign is a character. So we have to recode this variable to get rid of the %-sign and to convert the variable to a double precise floating number. This is similar procedure as we have already done with the Exercise variable. &gt; college1$Raven_Score_Code &lt;- college1$Raven_Score # copy column &gt; college1$Raven_Score_Code &lt;- as.double(gsub(&quot;%&quot;, &quot;&quot;, college1$Raven_Score_Code)) 3.5.4 Gender data Here I will reproduce the quote from the book In this data set (of the original Excel file, pb), Gender is coded as female = 1 and male = 2. ESCI calculates that the mean is M = 1.28 for Gender. Does this make sense? How might you better summarize gender in this sample? (p.69) No! It doesn’t make sense to calculate a mean for data on a nominal scale. It’s often convenient to represent nominal data with numerical codes, but it’s important to remember that these codes have no numerical meaning, and thus calculation of most descriptive statistics doesn’t make sense. Nominal data can be summarized with frequencies (175 females and 68 males) and relative frequencies (72.0% females and 28.0% males). (p.506) We can use the same program lines as we did with the college_survey1 data frame. &gt; n &lt;- table(college1$Gender) &gt; res &lt;- cbind(n, round(prop.table(n) * 100, 1)) &gt; colnames(res) &lt;- c(&quot;Count&quot;, &quot;%&quot;) &gt; res ## Count % ## Female 175 72 ## Male 68 28 We can also convert the variable Gender_Code or even better the variable Gender_Code into a factor variable. This is done with the forcats package which is already loaded as part of the tidyverse. When you hover with the mouse in the RStudio data browser over the column “Gender_Code” you will see the small message “column 3: numeric with range 1 - 2”. After running the following code try it again und you will see “column 3: factor with two levels”. &gt; college1$Gender_Code &lt;- as.factor(college1$Gender_Code) &gt; college1$Gender_Code ## [1] 1 2 1 2 1 2 2 1 1 2 1 2 1 1 1 1 1 1 1 2 1 1 2 2 1 1 2 1 1 1 1 1 2 1 1 2 1 1 1 1 2 2 2 2 1 2 1 1 2 1 2 1 1 ## [54] 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 2 1 2 2 2 1 1 1 2 1 2 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [107] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 2 1 1 2 2 1 2 1 2 1 1 1 2 1 1 1 2 1 2 2 2 2 1 1 2 1 2 1 1 1 1 1 2 1 ## [160] 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 2 1 2 1 2 1 1 2 1 1 1 1 1 1 2 1 1 1 1 1 1 2 1 1 2 2 1 1 1 1 1 1 ## [213] 2 2 1 2 1 2 1 1 2 2 1 2 1 1 1 1 1 1 1 2 1 1 1 1 2 1 2 2 2 2 1 ## Levels: 1 2 &gt; # remove first &#39;#&#39; from next line to get code working View(college1) # opens the data set in the RStudio data &gt; # browser We have successfully converted the numeric column into a categorial variable with 2 levels. All calculations with numbers will fail. &gt; mean(college1$Gender_Code) ## Warning in mean.default(college1$Gender_Code): argument is not numeric or logical: returning NA ## [1] NA "],
["end-of-chapter-exercises-1.html", "Chapter 4 End-of-Chapter Exercises 4.1 Personal remark 4.2 Find z scores 4.3 Find percentage of better students 4.4 Calculation of SE 4.5 Nursing home and random sampling 4.6 Skewed distributions 4.7 Warning sign for skewed variables 4.8 Additional challenge: 4.9 Getting data from a table on a web page 4.10 Tidy data 4.11 Display distribution for age at time of death", " Chapter 4 End-of-Chapter Exercises 4.1 Personal remark This is still a working draft of the tutorial part itns-solutions. I have this chapter yet to align with the new approach in chapter 1 und 2. Interactive exercises developed with H5P are still missing in this version I have almost always used the original text for questions and answers. To indicate these quotes are the text passages written in italics and marked with bar on the left margin. As there are only few R calculations in this chapter I have added an additional challenge: Drawing a distribution for age at time of death. Data are taken from a html table on the web. This results in three different exercises: Getting data via web scraping and cleaning the data frame Getting data via Excel file and cleaning the data frame Displaying the distribution with the program package ggplot2 4.2 Find z scores For a standardized exam of statistics skill, scores are normally distributed: μ = 80, σ = 5. Find each student’s z score: Student 1: X = 80 Student 2: X = 90 Student 3: X = 75 Student 4: X = 95 The formula is \\(z = (X-μ)/σ\\). &gt; (80 - 80)/5 # a. ## [1] 0 &gt; (90 - 80)/5 # b. ## [1] 2 &gt; (75 - 80)/5 # c. ## [1] -1 &gt; (95 - 80)/5 # d. ## [1] 3 \\(z = 0\\) \\(z = 2\\) \\(z = -1\\) \\(z = 3\\) 4.3 Find percentage of better students For each student in Exercise 1, use R to find what percent of students did better. (Assume X is a continuous variable.) I am using the pnorm command. You can get an explanation by using the help command help(pnorm) or help(Normal): &gt; help(Normal) &gt; (1 - pnorm(0)) * 100 ## [1] 50 &gt; (1 - pnorm(2)) * 100 ## [1] 2.275013 &gt; (1 - pnorm(-1)) * 100 ## [1] 84.13447 &gt; (1 - pnorm(3)) * 100 ## [1] 0.1349898 Percent better: a. 50%; b. 2.28%; c. 84.1%; d. 0.1%. 4.4 Calculation of SE Gabriela and Sylvia are working as a team for their university’s residential life program. They are both tasked with surveying students about their satisfaction with the dormitories. Today, Gabriela has managed to survey 25 students; Sylvia has managed to survey 36 students. The satisfaction scale they are using has a range from 1 to 20 and is known from previous surveys to have σ = 5. 4.4.1 Estimation 1 No mathematics, just think: which sample will have the smaller SE: the one collected by Gabriela or the one collected by Sylvia? Sylvia’s sample will have the smaller SE because she has collected a larger sample. 4.4.2 Estimation 2 When the two combine their data, will this shrink the SE or grow it? Combining the two samples will yield a smaller SE. 4.4.3 Calculation Now calculate the SE for Gabriela’s sample, for Sylvia’s sample, and for the two samples combined. The formula is \\(SE = σ / \\sqrt{N}\\). &gt; 5/sqrt(25) ## [1] 1 &gt; 5/sqrt(36) ## [1] 0.8333333 &gt; 5/sqrt(25 + 36) ## [1] 0.6401844 For Gabriela, SE = 1; For Sylvia, SE = 0.83; Combined, SE = 0.64. 4.4.4 Is the sample size sufficient? How big a sample size is needed? Based on the combined SE you obtained, does it seem like the residential life program should send Gabriela and Sylvia out to collect more data? Why or why not? This is a judgment call, but you should be able to make relevant comments. Consider not only the SE but the range of the measurement. What sample size is sufficient is a judgment call, which we’ll discuss further in Chapter 10. For now we can note that the combined data set provides SE = 0.64, meaning that many repeated samples would give sample mean satisfaction scores that would bounce around (i.e., form a mean heap) with standard deviation of 0.64. Given that satisfaction has a theoretical range from 1 to 20, this suggests that any one sample mean will provide a moderately precise estimate, reasonably close to the population mean. This analysis suggests we have sufficient data, although collecting more would of course most likely give us a better estimate. 4.5 Nursing home and random sampling Rebecca works at a nursing home. She’d like to study emotional intelligence amongst the seniors at the facility (her population of interest is all the seniors living at the facility). Which of these would represent random sampling for her study? Rebecca will wait in the lobby and approach any senior who randomly passes by. Rebecca will wait in the lobby. As a senior passes by she will flip a coin. If the coin lands heads she will ask the senior to be in the study, otherwise she will not. Rebecca will obtain a list of all the residents in the nursing home. She will randomly select 10% of the residents on this list; those selected will be asked to be part of the study. Rebecca will obtain a list of all the residents in the nursing home. She will randomly select 1% of the residents on this list; those selected will be asked to be part of the study. c and d represent random sampling because both give each member of the population an equal chance to be in the study, and members of the sample are selected independently 4.6 Skewed distributions Sampling distributions are not always normally distributed, especially when the variable measured is highly skewed. Below are some variables that tend to have strong skew. a) In real estate, home prices tend to be skewed. In which direction? Why might this be? b) Scores on easy tests tend to be skewed. In which direction? Why might this be? c) Age of death tends to be skewed. In which direction? Why might this be? d) Number of children in a family tends to be skewed. In which direction? Why might this be? ad a) Home prices tend to be positively skewed (longer tail to the right), because there is a lower boundary of zero, but in effect no maximum—typically a few houses have extremely high prices. These form the long upper tail of the distribution. ad b) Scores on an easy test tend to be negatively skewed (longer tail to the left). If the test is very easy, most scores will be piled up near the maximum, but there can still be a tail to the left representing a few students who scored poorly. ad c) Age at time of death tends to be negatively skewed (longer tail to the left). Death can strike at any time (☹), leading to a long lower tail; however, many people (in wealthy countries) die at around 70–85 years old, and no one lives forever, so the distribution is truncated at the upper end. Searching for “distribution of age at death”, or similar, will find you graphs showing this strong negative skew. What follows are two examples for this negatively skewed distributions of age at time of death: Figure 1: Celebrities death recorded by wikipedia: https://medium.com/@chris.wallace/was-2016-an-especially-bad-year-for-celebrity-deaths-40030e611f4f Figure 2: US-Distribution 2013 of age at time of death: https://www.quora.com/What-is-the-most-common-age-to-die-in-America ad d) Number of children in a family tends to be positively skewed (longer tail to the right) because 0 is a firm minimum, and then scores extend upward from there, with many families having, say, 1–4 children and a small number of families having many children. 4.7 Warning sign for skewed variables Based on the previous exercise, what is a caution or warning sign that a variable will be highly skewed? Anything that limits, filters, selects, or caps scores on the high or low end can lead to skew. Selection is not the only thing that can produce skew, but any time your participants have been subjected to some type of selection process you should be alert to the possibility of skew in the variables used to make the selection (and in any related variables). Also, if the mean and median differ by more than a small amount, most likely there is skew, with the mean being “pulled” in the direction of the longer tail. 4.8 Additional challenge: Both graphs above are calculated from data. The first one from data on wikipedia using python, the second one used data from a life table of the US social security administration. This opens up two questions for exercises: How to get data from websites and not especially prepared excel sheets? How to draw these above distributions? 4.9 Getting data from a table on a web page 4.9.1 Get table data with web scraping To get data from web pages is called web scraping. You will find with a search term line “R web scraping” many articles, tutorial and videos how to do it. Here I am going to use a blog post by Cory Nissen. We are going to use the R package rvest to write an appropriate script. Look at the US period life table from 2016: How to get the necessary data for the updates graph of figure 2? &gt; # install/load package `rvest` &gt; if (!require(&quot;rvest&quot;)) + {install.packages(&quot;rvest&quot;, repos = &#39;http://cran.wu.ac.at/&#39;) + library(rvest)} ## Loading required package: rvest ## Loading required package: xml2 ## ## Attaching package: &#39;rvest&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## pluck ## The following object is masked from &#39;package:readr&#39;: ## ## guess_encoding &gt; # store the web url of the page with the table you are interested in &gt; url &lt;- &quot;https://www.ssa.gov/oact/STATS/table4c6.html&quot; &gt; life_table_2016 &lt;- url %&gt;% + read_html() %&gt;% # from package xml2 + ## 1) go to webpage via google chrome + ## 2) set cursor on start of the desired table data + ## 3) right clicked and chose “inspect element” + ## 4) look for the appropiate line &lt;table …&gt; in the inspector (typically some lines above) + ## 5) select this &lt;table …&gt; line in the inspector + ## 6) right click it and select &quot;Copy -&gt; Copy XPath&quot; + ## 7) include the copied XPath into next line of the R script + html_nodes(xpath=&#39;//*[@id=&quot;content&quot;]/section[2]/div/div[3]/table&#39;) %&gt;% + html_table(fill = TRUE) &gt; life_table_2016 &lt;- life_table_2016[[1]] To clarify how to get the correct XPath data compare the following screenshots: Screenshot 1: Steps 1-3 of web scraping: Set cursor on start of the desired table data, click right mouse button to get the context menue and chose ‘inspect element’ Screenshot 2: Step 4 of web scraping: look for the appropiate line &lt;table …&gt; in the inspector (typically some lines above) 4.9.2 Copy table data into a Excel sheet Another simple way is to select the table on the web page (without header and footnotes) copy it into the clipboard fire up Excel set the cursor of the first cell of an empty Excel sheet paste the content of the clipboard into the Excel sheet save the data as a .csv file in your appropriate RStudio project folder Note: In order to save the data in the correct format you must use the (american) number format with ‘,’ as grouping character and ‘.’ as the decimal character. 4.10 Tidy data 4.10.1 Tidy data frame from web scraping Looking at the start and tail of the table we see that we need to make the following changes: We only need column 1, 3 and 6 for displaying the age distribution at time of the deaths. We need to delete the first and last row as it includes textual header information. We need to convert the character columns into columns containing numbers. We need to calculate the number of death from the survival numbers. &gt; # Looking at start and end of table &gt; head(life_table_2016) ## Exact age Male Male Male Female ## 1 Exact age Death\\r\\n probability a Number of\\r\\n lives b Life expectancy Death\\r\\n probability a ## 2 0 0.006364 100,000 76.04 0.005331 ## 3 1 0.000432 99,364 75.52 0.000359 ## 4 2 0.000284 99,321 74.55 0.000247 ## 5 3 0.000234 99,292 73.58 0.000169 ## 6 4 0.000170 99,269 72.59 0.000155 ## Female Female ## 1 Number of\\r\\n lives b Life\\r\\n expectancy ## 2 100,000 80.99 ## 3 99,467 80.43 ## 4 99,431 79.46 ## 5 99,407 78.48 ## 6 99,390 77.49 &gt; tail(life_table_2016) ## Exact age ## 117 115 ## 118 116 ## 119 117 ## 120 118 ## 121 119 ## 122 a Probability of dying within one year.\\r\\n b Number of survivors out of 100,000 born alive.\\r\\n Note:\\r\\n The period life expectancy at a given age for 2016 represents the average number\\r\\n of years of life remaining if a group of persons at that age were to experience\\r\\n the mortality rates for 2016 over the course of their remaining life. ## Male ## 117 0.732119 ## 118 0.768725 ## 119 0.807162 ## 120 0.847520 ## 121 0.889896 ## 122 a Probability of dying within one year.\\r\\n b Number of survivors out of 100,000 born alive.\\r\\n Note:\\r\\n The period life expectancy at a given age for 2016 represents the average number\\r\\n of years of life remaining if a group of persons at that age were to experience\\r\\n the mortality rates for 2016 over the course of their remaining life. ## Male ## 117 0 ## 118 0 ## 119 0 ## 120 0 ## 121 0 ## 122 a Probability of dying within one year.\\r\\n b Number of survivors out of 100,000 born alive.\\r\\n Note:\\r\\n The period life expectancy at a given age for 2016 represents the average number\\r\\n of years of life remaining if a group of persons at that age were to experience\\r\\n the mortality rates for 2016 over the course of their remaining life. ## Male ## 117 0.84 ## 118 0.78 ## 119 0.73 ## 120 0.67 ## 121 0.62 ## 122 a Probability of dying within one year.\\r\\n b Number of survivors out of 100,000 born alive.\\r\\n Note:\\r\\n The period life expectancy at a given age for 2016 represents the average number\\r\\n of years of life remaining if a group of persons at that age were to experience\\r\\n the mortality rates for 2016 over the course of their remaining life. ## Female ## 117 0.722682 ## 118 0.766043 ## 119 0.807162 ## 120 0.847520 ## 121 0.889896 ## 122 a Probability of dying within one year.\\r\\n b Number of survivors out of 100,000 born alive.\\r\\n Note:\\r\\n The period life expectancy at a given age for 2016 represents the average number\\r\\n of years of life remaining if a group of persons at that age were to experience\\r\\n the mortality rates for 2016 over the course of their remaining life. ## Female ## 117 0 ## 118 0 ## 119 0 ## 120 0 ## 121 0 ## 122 a Probability of dying within one year.\\r\\n b Number of survivors out of 100,000 born alive.\\r\\n Note:\\r\\n The period life expectancy at a given age for 2016 represents the average number\\r\\n of years of life remaining if a group of persons at that age were to experience\\r\\n the mortality rates for 2016 over the course of their remaining life. ## Female ## 117 0.86 ## 118 0.79 ## 119 0.73 ## 120 0.67 ## 121 0.62 ## 122 a Probability of dying within one year.\\r\\n b Number of survivors out of 100,000 born alive.\\r\\n Note:\\r\\n The period life expectancy at a given age for 2016 represents the average number\\r\\n of years of life remaining if a group of persons at that age were to experience\\r\\n the mortality rates for 2016 over the course of their remaining life. &gt; # We only need column 1, 3 and 6 and have to delete the last (text) line &gt; life_table_2016_v2 &lt;- life_table_2016[c(-1, -nrow(life_table_2016)), c(1, 3, 6)] &gt; &gt; # We need to convert the character columns into columns containing numbers In the columns with big number we &gt; # have additionally to delete all commas from the strings &gt; life_table_2016_v2$`Exact age` &lt;- as.numeric(life_table_2016_v2$`Exact age`) &gt; life_table_2016_v2$Male &lt;- as.numeric(gsub(&quot;,&quot;, &quot;&quot;, life_table_2016_v2$Male)) &gt; life_table_2016_v2$Female &lt;- as.numeric(gsub(&quot;,&quot;, &quot;&quot;, life_table_2016_v2$Female)) &gt; &gt; &gt; # We calculate the difference between rows &gt; life_table_2016_v3 &lt;- abs(tail(life_table_2016_v2[, -1], -1) - head(life_table_2016_v2[, -1], -1)) &gt; names(life_table_2016_v3)[1] &lt;- &quot;MaleDeath&quot; &gt; names(life_table_2016_v3)[2] &lt;- &quot;FemaleDeath&quot; &gt; &gt; # To bind the two data frames we need the same number of rows &gt; firstLine &lt;- data.frame(0, 0) &gt; names(firstLine)[1] &lt;- &quot;MaleDeath&quot; &gt; names(firstLine)[2] &lt;- &quot;FemaleDeath&quot; &gt; &gt; life_table_2016_v3 &lt;- rbind(firstLine, life_table_2016_v3) &gt; lifetable_final &lt;- cbind(life_table_2016_v2, life_table_2016_v3) &gt; &gt; # For easier access change variable name &#39;Exact age&#39; to &#39;age&#39; &gt; names(lifetable_final)[1] &lt;- &quot;age&quot; 4.10.2 Tidy data from Excel sheet We need to inspect and tidy our .csv file. Follow the procedure below: Click in RStudio project folder the .csv file name and select “View File” from the drop down menu which appears under your cursor Insert a new line (row) at the top of the file and add column names as I did. Save the file. Click another time of the now changed .csv file name in your RStudio project folder: The same drop down menu opens up as before. This time select “Import Dataset…” A overlay window to import text data appears. But the format of the data are not correct. The reason is that we have to change the delimiter from comma to semicolon. Now you can see the table in the correct format. In the window in the right corner you can also see the R code which generated after you click the button “Import”. You can use the next time this code as a template for the automatic import via R script. You can now inspect the data in a new tab in your code window. In the console window you can also check for the applied source code and the generated messages. You will see how the package readr has the data parsed and estimated the column specification. &gt; # load .csv file into variable &gt; lifetable &lt;- read_delim(&quot;lifetable.csv&quot;, &quot;;&quot;, escape_double = FALSE, trim_ws = TRUE) ## Parsed with column specification: ## cols( ## age = col_double(), ## male_death_prob = col_double(), ## male_survivor = col_number(), ## male_life_expect = col_double(), ## female_death_prob = col_double(), ## female_survivor = col_number(), ## female_life_expect = col_double() ## ) &gt; # We only need column 1, 3 and 6 &gt; lifetable_v2 &lt;- lifetable[, c(1, 3, 6)] &gt; &gt; # We calculate the difference between rows &gt; lifetable_v3 &lt;- abs(tail(lifetable_v2[, -1], -1) - head(lifetable_v2[, -1], -1)) &gt; names(lifetable_v3)[1] &lt;- &quot;MaleDeath&quot; &gt; names(lifetable_v3)[2] &lt;- &quot;FemaleDeath&quot; &gt; &gt; # To bind the two data frames we need the same number of rows &gt; firstLine &lt;- data.frame(0, 0) &gt; names(firstLine)[1] &lt;- &quot;MaleDeath&quot; &gt; names(firstLine)[2] &lt;- &quot;FemaleDeath&quot; &gt; &gt; lifetable_v3 &lt;- rbind(firstLine, lifetable_v3) &gt; lifetable_final &lt;- cbind(lifetable_v2, lifetable_v3) 4.11 Display distribution for age at time of death &gt; df &lt;- reshape2::melt(lifetable_final[, c(1, 4, 5)], id.vars = &quot;age&quot;, variable.name = &quot;Gender&quot;) &gt; ggplot(df, aes(age, value)) + geom_line(aes(colour = Gender)) + labs(x = &quot;Age of death in years&quot;, y = &quot;Death per 100,000&quot;) + + scale_color_discrete(labels = c(&quot;Male&quot;, &quot;Female&quot;)) + theme(legend.position = c(0.9, 0.8)) "],
["qr-codes-of-exercises.html", "Chapter 5 QR Codes of Exercises 5.1 Chapter 1", " Chapter 5 QR Codes of Exercises 5.1 Chapter 1 Research Question: Video Meta-Analysis: Video Open Science: Video Glossary: Accordion Glossary: Drag words Glossary: Memory Game Glossary: Fill-in-the-Blanks Research Process: Drag &amp; Drop Sketch Research: Form Wizard Research Process: Summary Estimation Plan: Summary Explore Forest Plot: Image Hotspots Point Estimates in Forest Plot: Find Hotspots Reporting: Accordion Quiz 1 Quiz 2 Poll Intuitions: True/False Assessment "],
["references.html", "References", " References "]
]
